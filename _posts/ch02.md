# 花书概念
## 第2章 线性代数
 - 代价函数 cost function 18,53,55,56,63,75,83,94-96,106,107,110-117,119,121,146,149,150,154,169,170,174-181,193,194,197,324,362,375,411,420,428,429
 
    代价函数，也成损失函数，主要是用来衡量预测值与目标值的差异，寻找最优解。
 - 标量 scalar 19
 
    一个单独的数，如整数、浮点数等。
 - 向量 vector 19

    一个向量是一列数。
 - 索引 index of matrix 19,20

    $$\begin{bmatrix}x_1 \\ x_2 \\ ... \\ x_3 \end{bmatrix}$$
    如这个向量，其下标就是索引，多个下标也能组成索引。
 - 矩阵 matrix 20
 
    二维数组。
 - 行 row 20
 
    A中的某一横排元素，$A_{i,j}$，i固定。
 - 列 column 20

    A中的某一竖排元素，$A_{i,j}$，j固定。
 - 张量 tensor 20
 
    维度超过两维的数组。
 - 转置 transpose 20
 
    以对角线为轴做的镜像操作/调换元素操作。
 - 主对角线 main diagonal 20,22,25

    从左上角到右上角的对角线。
 - 广播 broadcasting 20
 
    如矩阵和向量相加：$C=A+b$，则向量b会和矩阵A的每一行进行相加，这种隐式地复制向量b到很多位置的方式，称为广播。
 - 矩阵乘积 matrix product 21
 
    矩阵相乘是矩阵运算之一，如则乘法操作为$$C_{i,j}=\sum_{k} A_{i,k}B_{k,j}$$
 - 元素对应乘积 element-wise product 21
 
    即是对应位置的元素相乘， 如$$C=A\bigodot B,$$即是$$C_{i,j}=A_{i,j}B_{i,j}$$
 - Hadamard乘积 Hadamard product 21
 
    即是元素对应乘，同上。
 - 点积 dot product 21,24,25,89,282,290,291
 
    两相同维度的向量点积，即$x^{T} y$,矩阵相乘里A的一行乘以B的一列，就是点积操作。
 - 矩阵逆 matrix Inversion 22,24
 
    如果存在$A^{-1}$，使得$A^{-1} A = T_n$，则$A^{-1}$称为矩阵$A$的逆。
 - 单位矩阵 identity matrix 22
 
    主对角线的元素全为1，其他元素全为0的矩阵。

 - 原点 origin 23

    元素都是零向量。
 - 线性组合 linear combination 23

    对于$\mathbf{A} x = b$,其实相当于列向量($\mathbf{A}_{:,i}$)朝着一个方向走多远($x_i$)，即$$\mathbf{A} x = \sum_{i} x_i \mathbf{A}_{:,i}.$$称其为线性组合。
 - 生成子空间 span 23,24,27,307

    每个向量乘以标量再加和，如$\sum_{i} c_i \mathbf{v}^{(i)}$就是一组向量的生成子空间。
 - 列空间 column space 23

    $\mathbf{b}$是A其中一个生成子空间，为列空间，也叫值域。
 - 值域 range 23,78

    同上。

 - 线性相关 linear dependence 23

    一组向量中任意一个能够表示成其他向量的线性组合，则是线性相关。
 - 线性无关 linearly independent 23

    一组向量都不能表示成其他向量的线性组合，则是线性无关。

 - 方阵 square 24
 
    行数与列数相等的矩阵。
 - 奇异的 singular 24

    列相关相关且所有列向量都是线性无关的矩阵。
 - 范数 norm 24

    $$\left \| x \right \|_p=\left( \sum_i \left | x_i \right |^p\right )^\frac{1}{p}$$
 - 三角不等式 triangle inequality 24

    $$f(x+y)\leqslant f(x)+f(y)$$
 - 欧几里得范数数 Euclidean norn 24

    当p=2时，$L^2$范数即是欧几里得范数。

 - 最大范数 max norm 25

    最大范数$L^\infty$表示向量中最大元素的绝对值，即取决于最大幅度的元素：$$\left \| x \right \|_\infty=\max_i \left | x_i \right |$$
 - Frobenius范数 Frobenius norm 25,29,31,32

    衡量矩阵大小的最常见做法是Frobenius范数，类似于欧几里得范数，其方法是：$$\left \| A \right \|_F=\sqrt{\sum_{i,j}A_{i,j}^2}$$
 - 对角矩阵 diagonal matrix 25

    除了主对角线上有非零元素外，其他位置都为0的矩阵。
 - 对称 symmetric 25

    转置和自己相等的矩阵，或者可以看出按照对角线折叠相等的矩阵。
 - 单位向量 unit vector 26

    具有单位范数的向量，即2范数为1.
 - 单位范数 unit norn 26,30,31

    范数为1.
 - 正交 orthogonal 26

    如果$x^\top y=0$，则这两个向量互相正交。
 - 标准正交 orthonormal 26,27

    两个向量互相正交且范数为1.
    
 - 特征向量 eigenvector 26

    与方阵A相乘后相当于对该向量进行缩放（非零向量v）：$Av=\lambda v$.
 - 特征值 eigenvalue 26

    $\lambda$是该向量的特征值
 - 左特征向量 left eigenvector 26

    $v^\top A=\lambda v^\top$
 - 右特征向量 right eigenvector 26

    同上面的特征向量。
 - 正定 positive definite 27

    所有特征值都是正数的矩阵。正定矩阵保证$x^\top Ax = 0\Rightarrow x=0$
 - 半正定 positive semidefinite 27

    所有特征值都是非负数的矩阵。半正定矩阵保证对于任意的x，$x^\top Ax \geqslant 0$
 - 负定 negative definite 27,58

    所有特征值都是负数的矩阵。

 - 半负定 negative semidefinite 27

    所有特征值都是非正数的矩阵。
 - 奇异值分解 singular value decomposition 28

    特征分解去分解矩阵时，可以把A写成：$$A=Vdiag(\lambda)V^{-1}$$
    
    奇异值分解是将一个矩阵分解为三个矩阵相乘的形式：$$A=UD\, V^\top$$
    
    A是$m\times n$阶矩阵，那么U是$m\times m$阶矩阵，D是$m\times n$阶矩阵，V是$\times n$阶矩阵。U和都定义为正交矩阵，D定义为对角矩阵。
    ![奇异值分解](https://gss3.bdstatic.com/-Po3dSag_xI4khGkpoWK1HF6hhy/baike/c0%3Dbaike80%2C5%2C5%2C80%2C26/sign=7ab730a8692762d09433acedc185639f/95eef01f3a292df533a5239eb6315c6035a873d3.jpg)
 - 奇异向量 singular vector 28,93,318

    U和V的列向量是矩阵A的奇异向量。
 - 奇异值 singular value 28

    对角矩阵D对角线上的元素称为矩阵A的奇异值。A的非零奇异值是$AA^\top$和$A^\top A$特征值的平方根。
 - 左奇异向量 left singular vector 28

    U的列向量为A的左奇异向量，A的左奇异向量是$AA^\top$的特征向量。
 - 右奇异向量 right singular vector 28

    V的列向量称为A的右奇异向量，A的右奇异向量是$A^\top A$的特征向量
 - moore-penrose 伪逆 moore-penrose pseudoinverse 28,62,72,75

    伪逆定义为：$$A^+=\lim_{\alpha\rightarrow 0}(A^\top A+\alpha I)^{-1}A^\top$$
    
    （如果矩阵的行数大于列数，则可能没有解；如果行数小于列数，可能有多个解。伪逆可以看看[伪逆-csdn博文](https://blog.csdn.net/you1314520me/article/details/78857759)）
 - 主成分分析 principal components analysis 30,31,33,92,150,416

    假设要降成d'维，则是对协方差矩阵$X^\top X$进行特征值分解，得到前d'个特征值，组合成特征向量。PCA其实可以说是SVD的一个包装，SVD可以得到两个方向的PCA，如果我们对A’A进行特征值的分解，只能得到一个方向的PCA。